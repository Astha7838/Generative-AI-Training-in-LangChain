{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Prompt Templates**\n",
    "In LangChain, a PromptTemplate is a reusable prompt structure where `{variables}` can be dynamically filled at runtime. This helps standardize prompts while making them flexible.\n",
    "- https://python.langchain.com/docs/concepts/prompt_templates/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Tell me a joke about cats')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "\n",
    "prompt = prompt_template.invoke({\"topic\": \"cats\"})\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me a joke about cats'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-cdf4ef372fef>:6: LangChainBetaWarning: The function `init_chat_model` is in beta. It is actively being worked on, so the API may change.\n",
      "  model = init_chat_model(model=\"gpt-3.5-turbo\")\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "\n",
    "model = init_chat_model(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was the cat sitting on the computer?\n",
      "\n",
      "Because it wanted to keep an eye on the mouse!\n"
     ]
    }
   ],
   "source": [
    "response = model.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who wins if Thanos fights Superman. Pick one and make some bullet points\n"
     ]
    }
   ],
   "source": [
    "# An example prompt with multiple input variables\n",
    "template = PromptTemplate.from_template(\n",
    "    \"Who wins if {character1} fights {character2}. Pick one and make some bullet points\"\n",
    ")\n",
    "\n",
    "prompt = template.invoke({\n",
    "                        'character1': 'Thanos',\n",
    "                        'character2': 'Superman'\n",
    "                        })\n",
    "\n",
    "print(prompt.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thanos wins because:\n",
      "\n",
      "1. Thanos possesses superhuman strength, durability, and agility, which are on par with or even superior to Superman's.\n",
      "2. Thanos is a skilled fighter and tactician, able to outmaneuver and outsmart his opponents in battle.\n",
      "3. Thanos wields the Infinity Gauntlet, which grants him near-limitless power and control over reality, time, space, and all aspects of the universe.\n",
      "4. Superman may have weaknesses such as kryptonite or magic that Thanos could exploit to gain the upper hand in the fight.\n"
     ]
    }
   ],
   "source": [
    "response = model.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ChatPromptTemplates**\n",
    "\n",
    "ChatPromptTemplate is an advanced version of PromptTemplate designed for multi-turn chat interactions in LangChain. It allows you to structure conversations with multiple roles (system, human, AI) while dynamically filling in placeholders.\n",
    "\n",
    "### When we have Human and System templates\n",
    "\n",
    "These prompt templates are used to format a list of messages. These \"templates\" consist of a list of templates themselves.\n",
    "\n",
    "- https://python.langchain.com/docs/concepts/prompt_templates/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    PromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template  = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a chef.\"),\n",
    "    (\"human\", \"Recomend a vegan dish?\"),\n",
    "    (\"ai\", \"Sweet & Sour Tofu\"),\n",
    "    (\"human\", \"{followup}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['followup']\n"
     ]
    }
   ],
   "source": [
    "print(template.input_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You are a chef.'), HumanMessage(content='Recomend a vegan dish?'), AIMessage(content='Sweet & Sour Tofu'), HumanMessage(content=\"What's its recepie?\")]\n"
     ]
    }
   ],
   "source": [
    "prompt = template.invoke({\"followup\": \"What's its recepie?\"})\n",
    "print(prompt.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a chef.'),\n",
       " HumanMessage(content='Recomend a vegan dish?'),\n",
       " AIMessage(content='Sweet & Sour Tofu'),\n",
       " HumanMessage(content=\"What's its recepie?\")]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here is a recipe for Sweet & Sour Tofu:\n",
      "\n",
      "Ingredients:\n",
      "- 1 block of firm tofu, drained and cubed\n",
      "- 1 red bell pepper, sliced\n",
      "- 1 green bell pepper, sliced\n",
      "- 1 small onion, sliced\n",
      "- 1 cup pineapple chunks\n",
      "- 3 cloves of garlic, minced\n",
      "- 1/3 cup of rice vinegar\n",
      "- 1/4 cup of soy sauce\n",
      "- 1/4 cup of ketchup\n",
      "- 1/4 cup of brown sugar\n",
      "- 1 tablespoon of cornstarch\n",
      "- 2 tablespoons of vegetable oil\n",
      "- Salt and pepper to taste\n",
      "\n",
      "Instructions:\n",
      "1. In a small bowl, mix together the rice vinegar, soy sauce, ketchup, brown sugar, and cornstarch until well combined. Set aside.\n",
      "\n",
      "2. Heat the vegetable oil in a large skillet over medium-high heat. Add the tofu cubes and cook until golden brown on all sides. Remove the tofu from the skillet and set aside.\n",
      "\n",
      "3. In the same skillet, add the sliced bell peppers, onion, and garlic. Cook until the vegetables are tender-crisp.\n",
      "\n",
      "4. Add the pineapple chunks and the sauce mixture to the skillet. Stir well to combine.\n",
      "\n",
      "5. Return the tofu to the skillet and gently stir to coat the tofu with the sauce. Cook for an additional 2-3 minutes, until the sauce has thickened.\n",
      "\n",
      "6. Season with salt and pepper to taste.\n",
      "\n",
      "7. Serve the Sweet & Sour Tofu over steamed rice and enjoy!\n",
      "\n",
      "This dish is a delicious and satisfying vegan option that is sure to please your taste buds!\n"
     ]
    }
   ],
   "source": [
    "result = model.invoke(prompt)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex. 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['cooking_time', 'dietary_preference', 'recipe_request'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['cooking_time', 'dietary_preference'], template='You are an AI recipe assistant that specializes in {dietary_preference} dishes that can be prepared in {cooking_time}.')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['recipe_request'], template='{recipe_request}'))])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "system_template=\"You are an AI recipe assistant that specializes in {dietary_preference} dishes that can be prepared in {cooking_time}.\"\n",
    "human_template=\"{recipe_request}\"\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([(\"system\", system_template), \n",
    "                                                (\"user\", human_template)])\n",
    "\n",
    "template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cooking_time', 'dietary_preference', 'recipe_request']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are an AI recipe assistant that specializes in Vegan dishes that can be prepared in 15 min.'), HumanMessage(content='Quick Snack')])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = template.invoke({\n",
    "                    'cooking_time': \"15 min\",\n",
    "                    'dietary_preference': \"Vegan\",\n",
    "                    'recipe_request' :\"Quick Snack\"\n",
    "})\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are an AI recipe assistant that specializes in Vegan dishes that can be prepared in 15 min.'),\n",
       " HumanMessage(content='Quick Snack')]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an AI recipe assistant that specializes in Vegan dishes that can be prepared in 15 min.\n"
     ]
    }
   ],
   "source": [
    "print(prompt.messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How about making a simple and quick snack like avocado toast?\n",
      "\n",
      "Ingredients:\n",
      "- 1 ripe avocado\n",
      "- 2 slices of bread\n",
      "- Salt and pepper\n",
      "- Optional toppings like cherry tomatoes, red pepper flakes, or sesame seeds\n",
      "\n",
      "Instructions:\n",
      "1. Toast the bread slices until golden brown.\n",
      "2. Mash the ripe avocado in a bowl and season with salt and pepper.\n",
      "3. Spread the mashed avocado on top of the toasted bread slices.\n",
      "4. Add your favorite toppings like cherry tomatoes, red pepper flakes, or sesame seeds.\n",
      "5. Enjoy your delicious and nutritious avocado toast!\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Few-shot prompt templates:**\n",
    "\n",
    "A Few-Shot PromptTemplate in LangChain is a structured way to provide examples to the AI model before asking it to generate a response. This technique improves accuracy and consistency by showing the AI how to respond before it makes a prediction.\n",
    "\n",
    "\n",
    "- https://python.langchain.com/docs/concepts/few_shot_prompting/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "# Define examples\n",
    "examples = [\n",
    "    {\"word\": \"sport\", \"examples\": \"Soccer, Football, MMA\"},\n",
    "    {\"word\": \"food\", \"examples\": \"Pizza, Steak, Pasta\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['examples', 'word'], template='Word: {word}\\nExamples: {examples}\\n')"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define example format\n",
    "example_prompt = PromptTemplate.from_template(\n",
    "    \"Word: {word}\\nExamples: {examples}\\n\"\n",
    ")\n",
    "\n",
    "example_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Word: sport\\nExamples: Soccer, Football, MMA\\n')"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_prompt.invoke(examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='Word: sport\\nExamples: Soccer, Football, MMA\\n\\n\\nWord: food\\nExamples: Pizza, Steak, Pasta\\n\\n\\nWord: technology\\nExamples:'\n"
     ]
    }
   ],
   "source": [
    "# Create Few-Shot PromptTemplate\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Word: {word}\\nExamples:\",  # The final prompt template where AI fills in {word}.\n",
    "    input_variables=[\"word\"]\n",
    ")\n",
    "\n",
    "# Generate a formatted few-shot prompt for a new word\n",
    "formatted_prompt = few_shot_prompt.invoke({\"word\": \"technology\"})\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smartphones, Computers, Virtual Reality\n"
     ]
    }
   ],
   "source": [
    "result = model.invoke(formatted_prompt)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myEnv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
